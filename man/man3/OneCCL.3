.TH "OneCCLvars" 3 "Tue Sep 17 2024" "Version 2021.14.0" "IntelÂ® oneAPI Collective Communications Library" \" -*- nroff -*-
.ad l
.nh
.SH NAME
OneCCLvars
.SH SYNOPSIS
.br
.PP
.SS "Variables"

.in +1c
.ti -1c
.RI "\fBCCL_LOG_LEVEL\fP"
.br
.RI "Set this environment variable to control logging level\&. "
.ti -1c
.RI "\fBCCL_WORKER_COUNT\fP"
.br
.RI "Set specify the number of oneCCL worker threads\&. "
.ti -1c
.RI "\fBCCL_WORKER_AFFINITY\fP"
.br
.RI "Set to specify cpu affinity for oneCCL worker threads\&. "
.ti -1c
.RI "\fBCCL_WORKER_MEM_AFFINITY\fP"
.br
.RI "Set to specify memory affinity for oneCCL worker threads\&. 
.br
\&. "
.ti -1c
.RI "\fBCCL_KVS_MODE\fP"
.br
.RI "Select the mechanism to collect ranks while creating a communicator\&. "
.ti -1c
.RI "\fBCCL_KVS_CONNECTION_TIMEOUT\fP"
.br
.RI "Set the timeout for setting up connections during kvs initialization\&. "
.ti -1c
.RI "\fBCCL_KVS_MPI_ALLGATHER\fP"
.br
.RI "Set whether to use MPI_Allgather or custom implementation while creating a communicator\&. "
.ti -1c
.RI "\fBCCL_KVS_USE_MPI_RANKS\fP"
.br
.RI "Set whether to use mpi ranks directly while creating a communicator\&. In general, MPI_COMM_WORLD rank does not have to be the same as CCL communicator rank\&. Therefore, we need to gather MPI rank of all participants while creating a communicator\&. If user decide to use MPI rank and CCL rank as same, then they can set this CVAR to 1, and CCL can skip gathering MPI rank of all participants and use the information that CCL rank is same as MPI rank while creating communicator\&. "
.ti -1c
.RI "\fBCCL_ATL_SHM\fP"
.br
.RI "Set this environment variable to enable the OFI shared memory provider for communication between ranks in the same node of host (CPU) buffers\&. 
.br
\&. "
.ti -1c
.RI "\fBCCL_ENABLE_AUTO_CACHE\fP"
.br
.RI "Set this environment variable to enable cache model automatically for synchronous collectives with direct algorithms\&. "
.ti -1c
.RI "\fBCCL_ALLGATHER\fP"
.br
.RI "Set allgather algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLGATHERV\fP"
.br
.RI "Set allgatherv algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLREDUCE\fP"
.br
.RI "Set allreduce algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLTOALL\fP"
.br
.RI "Set alltoall algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLTOALLV\fP"
.br
.RI "Set alltoallv algorithm\&. "
.ti -1c
.RI "\fBCCL_BARRIER\fP"
.br
.RI "Set barrier algorithm\&. "
.ti -1c
.RI "\fBCCL_BCAST\fP"
.br
.RI "Set broadcast algorithm\&. "
.ti -1c
.RI "\fBCCL_BROADCAST\fP"
.br
.RI "Set broadcast algorithm (send_buf, recv_buf) "
.ti -1c
.RI "\fBCCL_REDUCE\fP"
.br
.RI "Set reduce algorithm\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER\fP"
.br
.RI "Set reduce-scatter algorithm\&. "
.ti -1c
.RI "\fBCCL_RECV\fP"
.br
.RI "Set recv algorithm\&. "
.ti -1c
.RI "\fBCCL_SEND\fP"
.br
.RI "Set send algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLGATHER_SCALEOUT\fP"
.br
.RI "Set scaleout allgather algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLGATHERV_SCALEOUT\fP"
.br
.RI "Set scaleout allgatherv algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLREDUCE_SCALEOUT\fP"
.br
.RI "Set allreduce scaleout algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLTOALL_SCALEOUT\fP"
.br
.RI "Set alltoall scaleout algorithm\&. "
.ti -1c
.RI "\fBCCL_ALLTOALLV_SCALEOUT\fP"
.br
.RI "Set alltoallv scaleout algorithm\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCALEOUT\fP"
.br
.RI "Set reduce scaleout algorithm\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER_SCALEOUT\fP"
.br
.RI "Set reduce-scatter scaleout algorithm\&. "
.ti -1c
.RI "\fBCCL_ZE_TMP_BUF_SIZE\fP"
.br
.RI "Specifies the size of the intermediate buffer used by oneCCL for collective operations\&. "
.ti -1c
.RI "\fBCCL_RS_CHUNK_COUNT\fP"
.br
.RI "Set to specify maximum number of chunks for reduce_scatter phase in ring allreduce\&. "
.ti -1c
.RI "\fBCCL_RS_MIN_CHUNK_SIZE\fP"
.br
.RI "Set to specify minimum number of bytes in chunk for reduce_scatter phase in ring allreduce\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER_TOPO_READ\fP"
.br
.RI "Set this environment variable to select read or write based device-to-device data copy during the reduce_scatter stage of Allreduce, Reduce, and Reduce-Scatter collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_ZE_DEPS_SYNC\fP"
.br
.RI "Set this environment variable to 1 to enable synchronous dependencies processing for oneCCL operations\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER_MONOLITHIC_KERNEL\fP"
.br
.RI "Set this environment variable to enable compute kernels for Allreduce, Reduce, and Reduce-Scatter collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_ALLGATHERV_MONOLITHIC_PIPELINE_KERNEL\fP"
.br
.RI "Set this environment variable to enable compute kernels for Allgather collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_ALLTOALLV_MONOLITHIC_KERNEL\fP"
.br
.RI "Set this environment variable to enable compute kernels for Alltoall and Alltoallv collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_ALLGATHERV_PIPE_CHUNK_COUNT\fP"
.br
.RI "Set this environment variable to enable pipelining implementation for Allgatherv collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_ALLREDUCE_PIPE_CHUNK_COUNT\fP"
.br
.RI "Set this environment variable to enable pipelining implementation for Allreduce collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER_PIPE_CHUNK_COUNT\fP"
.br
.RI "Set this environment variable to enable pipelining implementation for Reduce_Scatter collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_REDUCE_PIPE_CHUNK_COUNT\fP"
.br
.RI "Set this environment variable to enable pipelining implementation for Reduce collectives using device (GPU) buffers\&. "
.ti -1c
.RI "\fBCCL_LOCAL_RANK\fP"
.br
.RI "Set this environment variable to specify the rank number of the current process in the local host\&. "
.ti -1c
.RI "\fBCCL_LOCAL_SIZE\fP"
.br
.RI "Set this environment variable to specify the total number of ranks on the local host\&. "
.ti -1c
.RI "\fBCCL_PROCESS_LAUNCHER\fP"
.br
.RI "Set this environment variable to specify the job launcher to use\&. "
.ti -1c
.RI "\fBCCL_ZE_CACHE_OPEN_IPC_HANDLES\fP"
.br
.RI "Set this environment variable to enable or disable the caching of IPC handles opened with zeMemOpenIpcHandle()\&. "
.ti -1c
.RI "\fBCCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD\fP"
.br
.RI "Set this environment variable to specify the per process threshold for caching IPC handles opened with zeMemOpenIpcHandle()\&. "
.ti -1c
.RI "\fBCCL_ZE_CACHE_GET_IPC_HANDLES_THRESHOLD\fP"
.br
.RI "Set this environment variable to enable or disable the caching of IPC handles obtained with zeMemGetIpcHandle()\&. "
.ti -1c
.RI "\fBCCL_ZE_CACHE_GET_IPC_HANDLES\fP"
.br
.RI "Set this environment variable to specify the per process threshold for caching IPC handles obtained with zeMemGetIpcHandle()\&. "
.ti -1c
.RI "\fBCCL_ZE_ENABLE_OVERSUBSCRIPTION_FALLBACK\fP"
.br
.RI "Set to enable oversubscription in topo fallback stage for all collectives\&. "
.ti -1c
.RI "\fBCCL_ZE_ENABLE_OVERSUBSCRIPTION_THROW\fP"
.br
.RI "Set to enable oversubscription throw for all collectives\&. "
.ti -1c
.RI "\fBCCL_DRMFD_DEV_RENDER_DIR_PATH\fP"
.br
.RI "Set the directory path for DRM render devices\&. "
.ti -1c
.RI "\fBCCL_DRMFD_DEV_RENDER_SUFFIX\fP"
.br
.RI "Set the suffix for DRM render device names\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SH "Variable Documentation"
.PP 
.SS "CCL_ALLGATHER"

.PP
Set allgather algorithm\&. ALLGATHER algorithms
.IP "\(bu" 2
direct Based on MPI_Iallgather
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
ring Alltoall-based algorithm
.IP "\(bu" 2
flat Alltoall-based algorithm
.IP "\(bu" 2
multi_bcast Series of broadcast operations with different root ranks
.IP "\(bu" 2
topo Topo scaleup algorithm
.PP
.PP
By-default: 'topo', if sycl and l0 are enabled, otherwise 'naive' for ofi or 'direct' for mpi; 'ring' used as fallback 
.SS "CCL_ALLGATHER_SCALEOUT"

.PP
Set scaleout allgather algorithm\&. ALLGATHER algorithms
.IP "\(bu" 2
direct Based on MPI_Iallgather
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
ring Alltoall-based algorithm
.IP "\(bu" 2
flat Alltoall-based algorithm
.IP "\(bu" 2
multi_bcast Series of broadcast operations with different root ranks
.PP
.PP
By-default: 'naive' for ofi or 'direct' for mpi; 'ring' used as fallback 
.SS "CCL_ALLGATHERV"

.PP
Set allgatherv algorithm\&. ALLGATHERV algorithms
.IP "\(bu" 2
direct Based on MPI_Iallgatherv
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
ring Alltoall-based algorithm
.IP "\(bu" 2
flat Alltoall-based algorithm
.IP "\(bu" 2
multi_bcast Series of broadcast operations with different root ranks
.IP "\(bu" 2
topo Topo scaleup algorithm
.PP
.PP
By-default: 'topo', if sycl and l0 are enabled, otherwise 'naive' for ofi or 'direct' for mpi; 'ring' used as fallback 
.SS "CCL_ALLGATHERV_MONOLITHIC_PIPELINE_KERNEL"

.PP
Set this environment variable to enable compute kernels for Allgather collectives using device (GPU) buffers\&. Syntax
.PP
CCL_ALLGATHERV_MONOLITHIC_PIPELINE_KERNEL='<value>' Arguments
.PP
'<value>' Description
.IP "\(bu" 2
1 Uses compute kernels to transfer data across GPUs for Allgatherv collectives
.IP "\(bu" 2
0 Uses copy engines to transfer data across GPUs for Allgatherv collectives (default)
.PP
.PP
Description
.PP
Set this environment variable to enable compute kernels for Allgatherv collectives using device (GPU) buffers
.PP
By-default: '0' 
.SS "CCL_ALLGATHERV_PIPE_CHUNK_COUNT"

.PP
Set this environment variable to enable pipelining implementation for Allgatherv collectives using device (GPU) buffers\&. Syntax
.PP
CCL_ALLGATHERV_PIPE_CHUNK_COUNT='<value>' Arguments
.PP
'<value>' Description
.IP "\(bu" 2
0: (default) Bypasses the chunking/pipelining code and directly calls the topology-aware code
.IP "\(bu" 2
1: Calls the pipelining code with a single chunk\&. Effectively, it has identical behavior and performance as with '0', but exercises the chunking code path with a single chunk\&.
.IP "\(bu" 2
2 or higher: Divides the message into as many logical parts, or chunks, as specified\&. Then, it executes the collective with each logical chunk\&. This should allow for several phases of the algorithm to run in parallel, as long as they don't use the same physical resource\&. Effectively, this should increase performance\&.
.PP
.PP
Description
.PP
Set this environment variable to enable control how many chunks are used for Allgatherv, pipeline-based collectives using device (GPU) buffers\&.
.PP
By-default: '0' 
.SS "CCL_ALLGATHERV_SCALEOUT"

.PP
Set scaleout allgatherv algorithm\&. ALLGATHERV algorithms
.IP "\(bu" 2
direct Based on MPI_Iallgatherv
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
ring Alltoall-based algorithm
.IP "\(bu" 2
flat Alltoall-based algorithm
.IP "\(bu" 2
multi_bcast Series of broadcast operations with different root ranks
.PP
.PP
By-default: 'naive' for ofi or 'direct' for mpi; 'ring' used as fallback 
.SS "CCL_ALLREDUCE"

.PP
Set allreduce algorithm\&. ALLREDUCE algorithms
.IP "\(bu" 2
direct Based on MPI_Iallreduce
.IP "\(bu" 2
rabenseifner Rabenseifnerâs algorithm
.IP "\(bu" 2
nreduce May be beneficial for imbalanced workloads
.IP "\(bu" 2
ring Reduce_scatter + allgather ring\&. Use CCL_RS_CHUNK_COUNT and CCL_RS_MIN_CHUNK_SIZE to control pipelining on reduce_scatter phase\&.
.IP "\(bu" 2
double_tree Double-tree algorithm
.IP "\(bu" 2
recursive_doubling Recursive doubling algorithm
.IP "\(bu" 2
2d Two-dimensional algorithm (reduce_scatter + allreduce + allgather)\&. Only available for Host (CPU) buffers\&.
.IP "\(bu" 2
topo Topo scaleup algorithm (available if sycl and l0 are enabled)
.PP
.PP
By-default: 'topo', if sycl and l0 are enable, otherwise 'ring' 
.SS "CCL_ALLREDUCE_PIPE_CHUNK_COUNT"

.PP
Set this environment variable to enable pipelining implementation for Allreduce collectives using device (GPU) buffers\&. Syntax
.PP
CCL_ALLREDUCE_PIPE_CHUNK_COUNT='<value>' Arguments
.PP
'<value>' Description
.IP "\(bu" 2
0: (default) Bypasses the chunking/pipelining code and directly calls the topology-aware code
.IP "\(bu" 2
1: Calls the pipelining code with a single chunk\&. Effectively, it has identical behavior and performance as with '0', but exercises the chunking code path with a single chunk\&.
.IP "\(bu" 2
2 or higher: Divides the message into as many logical parts, or chunks, as specified\&. Then, it executes the collective with each logical chunk\&. This should allow for several phases of the algorithm to run in parallel, as long as they don't use the same physical resource\&. Effectively, this should increase performance\&.
.PP
.PP
Description
.PP
Set this environment variable to enable control how many chunks are used for Allreduce pipeline-based collectives using device (GPU) buffers\&.
.PP
By-default: '0' 
.SS "CCL_ALLREDUCE_SCALEOUT"

.PP
Set allreduce scaleout algorithm\&. ALLREDUCE algorithms
.IP "\(bu" 2
direct Based on MPI_Iallreduce
.IP "\(bu" 2
rabenseifner Rabenseifnerâs algorithm
.IP "\(bu" 2
nreduce May be beneficial for imbalanced workloads
.IP "\(bu" 2
ring Reduce_scatter + allgather ring\&. Use CCL_RS_CHUNK_COUNT and CCL_RS_MIN_CHUNK_SIZE to control pipelining on reduce_scatter phase\&.
.IP "\(bu" 2
double_tree Double-tree algorithm
.IP "\(bu" 2
recursive_doubling Recursive doubling algorithm
.IP "\(bu" 2
2d Two-dimensional algorithm (reduce_scatter + allreduce + allgather)\&. Only available for Host (CPU) buffers\&.
.PP
.PP
By-default: 'ring' 
.SS "CCL_ALLTOALL"

.PP
Set alltoall algorithm\&. ALLTOALLV algorithms
.IP "\(bu" 2
direct Based on MPI_Ialltoallv
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
scatter Scatter-based algorithm
.IP "\(bu" 2
topo Topo scaleup algorithm (available if sycl and l0 are enabled)
.PP
.PP
By-default: 'topo', if sycl and l0 are enable, otherwise 'scatter' 
.SS "CCL_ALLTOALL_SCALEOUT"

.PP
Set alltoall scaleout algorithm\&. ALLTOALL algorithms
.IP "\(bu" 2
direct Based on MPI_Ialltoall
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
scatter Scatter-based algorithm
.PP
.PP
By-default: 'scatter' 
.SS "CCL_ALLTOALLV"

.PP
Set alltoallv algorithm\&. ALLTOALLV algorithms
.IP "\(bu" 2
direct Based on MPI_Ialltoallv
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
topo Topo scaleup algorithm (available if sycl and l0 are enabled)
.PP
.PP
By-default: 'topo', if sycl and l0 are enable, otherwise 'scatter' 
.SS "CCL_ALLTOALLV_MONOLITHIC_KERNEL"

.PP
Set this environment variable to enable compute kernels for Alltoall and Alltoallv collectives using device (GPU) buffers\&. Syntax
.PP
CCL_ALLTOALLV_MONOLITHIC_KERNEL='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
1 Uses compute kernels to transfer data across GPUs for AlltoAll and Alltoallv collectives (default)
.IP "\(bu" 2
0 Uses copy engines to transfer data across GPUs for AlltoAll and Alltoallv collectives
.PP
.PP
Description
.PP
Set this environment variable to enable compute kernels for Alltoall and Alltoallv collectives using device (GPU) buffers
.PP
By-default: '1' 
.SS "CCL_ALLTOALLV_SCALEOUT"

.PP
Set alltoallv scaleout algorithm\&. ALLTOALLV algorithms
.IP "\(bu" 2
direct Based on MPI_Ialltoallv
.IP "\(bu" 2
naive Send to all, receive from all
.IP "\(bu" 2
scatter Scatter-based algorithm
.PP
.PP
By-default: 'scatter' 
.SS "CCL_ATL_SHM"

.PP
Set this environment variable to enable the OFI shared memory provider for communication between ranks in the same node of host (CPU) buffers\&. 
.br
\&. Syntax 
.br
CCL_ATL_SHM='<value>'
.br

.br
Arguments
.br
'<value>' Description
.br
.IP "\(bu" 2
0 Disables OFI shared memory provider (default)\&.
.br

.IP "\(bu" 2
1 Enables OFI shared memory provider\&.
.br

.br
Description
.br
 Set this environment variable to enable the OFI shared memory provider for communication between ranks in the same node of host (CPU) buffers\&.
.PP
.PP
By-default: '0' 
.SS "CCL_BARRIER"

.PP
Set barrier algorithm\&. BARRIER algorithms
.IP "\(bu" 2
direct Based on MPI_Ibarrier
.IP "\(bu" 2
ring Ring-based algorithm
.PP
.PP
Note: BARRIER does not support the CCL_BARRIER_SCALEOUT environment variable\&. To change the algorithm for scaleout, use CCL_BARRIER\&.
.PP
By-default: 'direct' 
.SS "CCL_BCAST"

.PP
Set broadcast algorithm\&. BCAST algorithms
.IP "\(bu" 2
direct Based on MPI_Ibcast
.IP "\(bu" 2
ring Ring
.IP "\(bu" 2
double_tree Double-tree algorithm
.IP "\(bu" 2
naive Send to all from root rank
.PP
.PP
Note: BCAST algorithm does not support yet the CCL_BCAST_SCALEOUT environment variable\&. To change the algorithm for BCAST, use CCL_BCAST\&.
.PP
By-default: 'direct' 
.SS "CCL_BROADCAST"

.PP
Set broadcast algorithm (send_buf, recv_buf) BCAST algorithms
.IP "\(bu" 2
direct Based on MPI_Ibcast
.IP "\(bu" 2
ring Ring
.IP "\(bu" 2
double_tree Double-tree algorithm
.IP "\(bu" 2
naive Send to all from root rank
.PP
.PP
Note: BCAST algorithm does not support yet the CCL_BCAST_SCALEOUT environment variable\&. To change the algorithm for BCAST, use CCL_BCAST\&.
.PP
By-default: 'direct' 
.SS "CCL_DRMFD_DEV_RENDER_DIR_PATH"

.PP
Set the directory path for DRM render devices\&. This environment variable specifies the directory path where DRM render devices are located\&.
.PP
Example value: '/custom/path/to/devices/'
.PP
By-default: '/dev/dri/by-path/' 
.SS "CCL_DRMFD_DEV_RENDER_SUFFIX"

.PP
Set the suffix for DRM render device names\&. This environment variable specifies the suffix to be used when searching for DRM render device names\&.
.PP
Example value: '-customsuffix'
.PP
By-default: '-render' 
.SS "CCL_ENABLE_AUTO_CACHE"

.PP
Set this environment variable to enable cache model automatically for synchronous collectives with direct algorithms\&. Syntax 
.br
CCL_ENABLE_AUTO_CACHE='<value>'
.br

.br
Arguments
.br
'<value>' Description
.br
.IP "\(bu" 2
0 Does not allow enabling cache model automatically\&.
.br

.IP "\(bu" 2
1 Allows enabling cache model automatically (default)\&.
.br

.br

.PP
.PP
By-default: '1' 
.SS "CCL_KVS_CONNECTION_TIMEOUT"

.PP
Set the timeout for setting up connections during kvs initialization\&. '<timeout>' - Timeout in seconds to use for setting up sockets during kvs initialization
.PP
By-default: '120' 
.SS "CCL_KVS_MODE"

.PP
Select the mechanism to collect ranks while creating a communicator\&. '<value>': 
.br
'0' - use default implementation using sockets 
.br
'1' - use mpi 
.br
KVS implemention with sockets is used to collect the rank information while creating communicator by default\&. 
.br
 By-default: '0' 
.SS "CCL_KVS_MPI_ALLGATHER"

.PP
Set whether to use MPI_Allgather or custom implementation while creating a communicator\&. '<value>': 
.br
'0' - use only custom implementation of allgather 
.br
'1' - use MPI_Allgather whenever possible 
.br
 By-default: '1' 
.SS "CCL_KVS_USE_MPI_RANKS"

.PP
Set whether to use mpi ranks directly while creating a communicator\&. In general, MPI_COMM_WORLD rank does not have to be the same as CCL communicator rank\&. Therefore, we need to gather MPI rank of all participants while creating a communicator\&. If user decide to use MPI rank and CCL rank as same, then they can set this CVAR to 1, and CCL can skip gathering MPI rank of all participants and use the information that CCL rank is same as MPI rank while creating communicator\&. '<value>': 
.br
'0' - collect mpi ranks from everyone 
.br
'1' - use mpi ranks directly without collecting 
.br
 By-default: '0' 
.SS "CCL_LOCAL_RANK"

.PP
Set this environment variable to specify the rank number of the current process in the local host\&. Syntax
.PP
CCL_LOCAL_RANK='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
RANK Rank number of the current process in the local host
.PP
.PP
Description
.PP
Set this environment variable to specify the rank number of the current process in the local host
.PP
By-default: N/A; job/process launcher (CCL_PROCESS_LAUNCHER) needs to be used if variable not specified 
.SS "CCL_LOCAL_SIZE"

.PP
Set this environment variable to specify the total number of ranks on the local host\&. Syntax
.PP
CCL_LOCAL_SIZE='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
SIZE Total number of ranks on the local host\&.
.PP
.PP
Description
.PP
Set this environment variable to specify the total number of ranks on the local host
.PP
By-default: N/A; job/process launcher (CCL_PROCESS_LAUNCHER) needs to be used if variable not specified 
.SS "CCL_LOG_LEVEL"

.PP
Set this environment variable to control logging level\&. The \fCCCL_LOG_LEVEL\fP environment variable can be set to control the level of detail in the logging output generated by the CCL library\&.
.PP
'<value>': 'error', 'warn', 'info', 'debug', 'trace'
.PP
By-default: 'warn' 
.SS "CCL_PROCESS_LAUNCHER"

.PP
Set this environment variable to specify the job launcher to use\&. Syntax
.PP
CCL_PROCESS_LAUNCHER='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
hydra Uses the MPI hydra job launcher (default)
.IP "\(bu" 2
torchrun Uses torchrun job launcher
.IP "\(bu" 2
pmix It is used with the PALS job launcher which uses the pmix API, so your mpiexec command should look something like this: CCL_PROCESS_LAUNCHER=pmix CCL_ATL_TRANSPORT=mpi mpiexec -np 2 -ppn 2 --pmi=pmix \&.\&.\&.
.IP "\(bu" 2
none No Job launcher is used\&. In this case, the user needs to specify the values for CCL_LOCAL_SIZE and CCL_LOCAL_RANK
.PP
.PP
Description
.PP
Set this environment variable to specify the job launcher to use\&.
.PP
By-default: 'hydra' 
.SS "CCL_RECV"

.PP
Set recv algorithm\&. RECV algorithms
.IP "\(bu" 2
direct Using prepost(d2h-h2d) copies to get host buffers to invoke mpi/ofi->recv()
.IP "\(bu" 2
topo Topo scale-up algorithm (available if sycl and l0 are enabled)
.IP "\(bu" 2
offload Using device buffers directly into mpi/ofi layer skipping prepost copies d2h h2d\&. By-default used for scale-out\&. Setting extra MPI env vars for getting better performance (available if sycl and l0 are enabled)
.PP
.PP
By-default: 'topo' if sycl and l0 are enabled, otherwise offload for ofi/mpi transport 
.SS "CCL_REDUCE"

.PP
Set reduce algorithm\&. REDUCE algorithms
.IP "\(bu" 2
direct Based on MPI_Ireduce
.IP "\(bu" 2
rabenseifner Rabenseifnerâs algorithm
.IP "\(bu" 2
ring Ring algorithm
.IP "\(bu" 2
tree Tree algorithm
.IP "\(bu" 2
double_tree Double-tree algorithm
.IP "\(bu" 2
topo Topo scaleup algorithm (available if sycl and l0 are enabled)
.PP
.PP
By-default: 'topo' if sycl and l0 are enabled, otherwise tree for ofi transport or direct for mpi 
.SS "CCL_REDUCE_PIPE_CHUNK_COUNT"

.PP
Set this environment variable to enable pipelining implementation for Reduce collectives using device (GPU) buffers\&. Syntax
.PP
CCL_REDUCE_PIPE_CHUNK_COUNT='<value>' Arguments
.PP
'<value>' Description
.IP "\(bu" 2
0: (default) Bypasses the chunking/pipelining code and directly calls the topology-aware code
.IP "\(bu" 2
1: Calls the pipelining code with a single chunk\&. Effectively, it has identical behavior and performance as with '0', but exercises the chunking code path with a single chunk\&.
.IP "\(bu" 2
2 or higher: Divides the message into as many logical parts, or chunks, as specified\&. Then, it executes the collective with each logical chunk\&. This should allow for several phases of the algorithm to run in parallel, as long as they don't use the same physical resource\&. Effectively, this should increase performance\&.
.PP
.PP
Description
.PP
Set this environment variable to enable control how many chunks are used for Reduce pipeline-based collectives using device (GPU) buffers\&.
.PP
By-default: '0' 
.SS "CCL_REDUCE_SCALEOUT"

.PP
Set reduce scaleout algorithm\&. REDUCE algorithms
.IP "\(bu" 2
direct Based on MPI_Ireduce
.IP "\(bu" 2
rabenseifner Rabenseifnerâs algorithm
.IP "\(bu" 2
ring Ring algorithm
.IP "\(bu" 2
tree Tree algorithm
.IP "\(bu" 2
double_tree Double-tree algorithm
.PP
.PP
By-default: 'double_tree' 
.SS "CCL_REDUCE_SCATTER"

.PP
Set reduce-scatter algorithm\&. REDUCE_SCATTER algorithms
.IP "\(bu" 2
direct Based on MPI_Ireduce_scatter_block
.IP "\(bu" 2
naive Send to all, receive and reduce from all
.IP "\(bu" 2
ring Ring-based algorithm\&. Use CCL_RS_CHUNK_COUNT and CCL_RS_MIN_CHUNK_SIZE to control pipelining\&.
.IP "\(bu" 2
topo Topo algorithm (available if sycl and l0 are enabled, scaleup only)
.PP
.PP
By-default: 'topo' if sycl and l0 are enabled, otherwise naive for ofi transport or direct for mpi 
.SS "CCL_REDUCE_SCATTER_MONOLITHIC_KERNEL"

.PP
Set this environment variable to enable compute kernels for Allreduce, Reduce, and Reduce-Scatter collectives using device (GPU) buffers\&. Syntax CCL_REDUCE_SCATTER_MONOLITHIC_KERNEL='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
1 Uses compute kernels to transfer data across GPUs for Allreduce, Reduce, and Reduce-Scatter collectives
.IP "\(bu" 2
0 Uses copy engines to transfer data across GPUs for Allreduce, Reduce, and Reduce-Scatter collectives (default)\&.
.PP
.PP
Description
.PP
Set this environment variable to enable compute kernels for Allreduce, Reduce, and Reduce-Scatter collectives using device (GPU) buffers
.PP
By-default: '0' 
.SS "CCL_REDUCE_SCATTER_PIPE_CHUNK_COUNT"

.PP
Set this environment variable to enable pipelining implementation for Reduce_Scatter collectives using device (GPU) buffers\&. Syntax
.PP
CCL_REDUCE_SCATTER_PIPE_CHUNK_COUNT='<value>' Arguments
.PP
'<value>' Description
.IP "\(bu" 2
0: (default) Bypasses the chunking/pipelining code and directly calls the topology-aware code
.IP "\(bu" 2
1: Calls the pipelining code with a single chunk\&. Effectively, it has identical behavior and performance as with '0', but exercises the chunking code path with a single chunk\&.
.IP "\(bu" 2
2 or higher: Divides the message into as many logical parts, or chunks, as specified\&. Then, it executes the collective with each logical chunk\&. This should allow for several phases of the algorithm to run in parallel, as long as they don't use the same physical resource\&. Effectively, this should increase performance\&.
.PP
.PP
Description
.PP
Set this environment variable to enable control how many chunks are used for Reduce_Scatter pipeline-based collectives using device (GPU) buffers\&.
.PP
By-default: '0' 
.SS "CCL_REDUCE_SCATTER_SCALEOUT"

.PP
Set reduce-scatter scaleout algorithm\&. REDUCE_SCATTER algorithms
.IP "\(bu" 2
direct Based on MPI_Ireduce_scatter_block
.IP "\(bu" 2
naive Send to all, receive and reduce from all
.IP "\(bu" 2
ring Ring-based algorithm\&. Use CCL_RS_CHUNK_COUNT and CCL_RS_MIN_CHUNK_SIZE to control pipelining\&.
.PP
.PP
By-default: 'naive' 
.SS "CCL_REDUCE_SCATTER_TOPO_READ"

.PP
Set this environment variable to select read or write based device-to-device data copy during the reduce_scatter stage of Allreduce, Reduce, and Reduce-Scatter collectives using device (GPU) buffers\&. Syntax CCL_REDUCE_SCATTER_TOPO_READ='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
1 Uses read based copy to transfer data across GPUs for the reduce_scatter stage of Allreduce, Reduce, and Reduce-Scatter collectives (default)\&.
.IP "\(bu" 2
0 Uses write based copy to transfer data across GPUs for the reduce_scatter stage of Allreduce, Reduce, and Reduce-Scatter collectives\&.
.PP
.PP
Description
.PP
Set this environment variable to select read or write based device-to-device data copy during the reduce_scatter stage of Allreduce, Reduce, and Reduce-Scatter collectives using device (GPU) buffers\&.
.PP
By-default: '1' 
.SS "CCL_RS_CHUNK_COUNT"

.PP
Set to specify maximum number of chunks for reduce_scatter phase in ring allreduce\&. '<count>' - Maximum number of chunks for reduce_scatter phase in ring allreduce
.PP
By-default: '1' 
.SS "CCL_RS_MIN_CHUNK_SIZE"

.PP
Set to specify minimum number of bytes in chunk for reduce_scatter phase in ring allreduce\&. '<size>' - Minimum number of bytes in chunk for reduce_scatter phase in ring allreduce\&. Affects actual value of CCL_RS_CHUNK_COUNT\&.
.PP
By-default: '65536' 
.SS "CCL_SEND"

.PP
Set send algorithm\&. SEND algorithms
.IP "\(bu" 2
direct Using prepost(d2h-h2d) copies to get host buffers to invoke mpi/ofi->send()
.IP "\(bu" 2
topo Topo scale-up algorithm (available if sycl and l0 are enabled)
.IP "\(bu" 2
offload Using device buffers directly into mpi/ofi layer skipping prepost copies d2h h2d\&. By-default used for scale-out\&. Setting extra MPI env vars for getting better performance (available if sycl and l0 are enabled)
.PP
.PP
By-default: 'topo' if sycl and l0 are enabled, otherwise offload for ofi/mpi transport 
.SS "CCL_WORKER_AFFINITY"

.PP
Set to specify cpu affinity for oneCCL worker threads\&. '<value>': 'auto', '<cpulist>': 
.br
 'auto' - Workers are automatically pinned to last cores of pin domain\&. Pin domain depends from process launcher\&. If mpirun from oneCCL package is used then pin domain is MPI process pin domain\&. Otherwise, pin domain is all cores on the node\&. 
.br
 '<cpulist>' - A comma-separated list of core numbers and/or ranges of core numbers for all local workers, one number per worker\&. The i-th local worker is pinned to the i-th core in the list\&. For example 'a','b'-'c'defines list of cores contaning core with number 'a' and range of cores with numbers from 'b' to 'c'\&. The number should not exceed the number of cores available on the system\&.
.PP
By-default: 'not-specified' 
.SS "CCL_WORKER_COUNT"

.PP
Set specify the number of oneCCL worker threads\&. '<value>' - The number of worker threads for oneCCL rank
.PP
By-default: '1' 
.SS "CCL_WORKER_MEM_AFFINITY"

.PP
Set to specify memory affinity for oneCCL worker threads\&. 
.br
\&. '<nodelist>' : 
.br
 'auto' - Workers are automatically pinned to NUMA nodes that correspond to CPU affinity of workers\&. 
.br
 A comma-separated list of NUMA node numbers for all local workers, one number per worker\&. The i-th local worker is pinned to the i-th NUMA node in the list\&. The number should not exceed the number of NUMA nodes available on the system\&.
.PP
By-default: 'not-specified' 
.SS "CCL_ZE_CACHE_GET_IPC_HANDLES"

.PP
Set this environment variable to specify the per process threshold for caching IPC handles obtained with zeMemGetIpcHandle()\&. This controls whether it caches IPC handles obtained with zeMemGetIpcHandle() on sender's side\&. When enabled, it caches IPC handles, which can improve performance in certain scenarios\&. By default, the caching of get IPC handles is enabled\&. See https://spec.oneapi.io/level-zero/latest/core/PROG.html#memory-1
.PP
CCL_ZE_CACHE_GET_IPC_HANDLES='<value>'
.PP
'<value>'
.IP "\(bu" 2
0 Disables the caching of get IPC handles\&.
.IP "\(bu" 2
1 Enables the caching of get IPC handles (default)\&.
.PP
.PP
By-default: '1' 
.SS "CCL_ZE_CACHE_GET_IPC_HANDLES_THRESHOLD"

.PP
Set this environment variable to enable or disable the caching of IPC handles obtained with zeMemGetIpcHandle()\&. This environment variable specifies the threshold for caching get IPC handles on sender's side\&. When the number of IPC handles obtained with zeMemGetIpcHandle() exceeds this threshold, the cache will start evicting handles via LRU from the cache\&.
.PP
CCL_ZE_CACHE_GET_IPC_HANDLES_THRESHOLD='<value>'
.PP
'<value>'
.IP "\(bu" 2
SIZE The threshold value for caching get IPC handles\&.
.PP
.PP
By-default: '1000' 
.SS "CCL_ZE_CACHE_OPEN_IPC_HANDLES"

.PP
Set this environment variable to enable or disable the caching of IPC handles opened with zeMemOpenIpcHandle()\&. This controls whether it caches IPC handles opened with zeMemOpenIpcHandle() on receiver's side\&. When enabled, it caches opened IPC handles, which can improve performance in certain scenarios\&. See https://spec.oneapi.io/level-zero/latest/core/PROG.html#memory-1
.PP
CCL_ZE_CACHE_OPEN_IPC_HANDLES='<value>'
.PP
'<value>'
.IP "\(bu" 2
0 Disables the caching of opened IPC handles\&.
.IP "\(bu" 2
1 Enables the caching of opened IPC handles (default)\&.
.PP
.PP
By-default: '1' 
.SS "CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD"

.PP
Set this environment variable to specify the per process threshold for caching IPC handles opened with zeMemOpenIpcHandle()\&. This specifies the threshold for caching open IPC handles on receiver's side\&. When the number of open IPC handles exceeds this threshold, the cache will start evicting handles via LRU from the cache\&.
.PP
CCL_ZE_CACHE_OPEN_IPC_HANDLES_THRESHOLD='<value>'
.PP
'<value>'
.IP "\(bu" 2
SIZE The threshold value for caching open IPC handles\&.
.PP
.PP
By-default: '1000' 
.SS "CCL_ZE_DEPS_SYNC"

.PP
Set this environment variable to 1 to enable synchronous dependencies processing for oneCCL operations\&. Syntax CCL_ZE_DEPS_SYNC='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
1 Dependencies of oneCCL operations are processed synchronously\&.
.IP "\(bu" 2
0 Dependencies of oneCCL operations are processed asynchronously (default), meaning that further L0 submissions are being done while dependencies are in progress\&. Dependencies are signaling when processed\&.
.PP
.PP
Description
.PP
Set this environment variable to 1 to make oneCCL block the thread while previous sycl/L0 submissions are not finished\&.
.PP
By-default: '0' 
.SS "CCL_ZE_ENABLE_OVERSUBSCRIPTION_FALLBACK"

.PP
Set to enable oversubscription in topo fallback stage for all collectives\&. This enviroment variable enables or disables the oversubscription fallback from topo algorithm to copy in/out
.PP
'<value>' : '0', '1'
.PP
By-default: '1' 
.SS "CCL_ZE_ENABLE_OVERSUBSCRIPTION_THROW"

.PP
Set to enable oversubscription throw for all collectives\&. This enviroment variable enables or disables the oversubscription throw check
.PP
'<value>' : '0', '1'
.PP
By-default: '1' 
.SS "CCL_ZE_TMP_BUF_SIZE"

.PP
Specifies the size of the intermediate buffer used by oneCCL for collective operations\&. The CCL_ZE_TMP_BUF_SIZE environment variable controls the size of the buffer that is used for temporary buffers of collective operations in 'topo' algorithms\&. It has no effect on other algorithms\&. Smaller values can reduce memory usage at the expense of performance for 'topo' algorithms\&.
.PP
Syntax
.PP
CCL_ZE_TMP_BUF_SIZE='<value>'
.PP
Arguments
.PP
'<value>' Description
.IP "\(bu" 2
SIZE The size of the buffer in bytes\&.
.PP
.PP
By-default: '536870912' 
.SH "Author"
.PP 
Generated automatically by Doxygen for IntelÂ® oneAPI Collective Communications Library from the source code\&.
.TH "ExpOneCCLvars" 3 "Tue Sep 17 2024" "Version 2021.14.0" "IntelÂ® oneAPI Collective Communications Library" \" -*- nroff -*-
.ad l
.nh
.SH NAME
ExpOneCCLvars \- Experimental OneCCL Environment Variables Functionality of these variables has not been (fully) tested and, therefore, cannot be supported nor guaranteed\&.  

.SH SYNOPSIS
.br
.PP
.SS "Variables"

.in +1c
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER_MONOLITHIC_PIPELINE_KERNEL\fP"
.br
.RI "Set to specify monolithic pipeline approach for reduce_scatter phase in allreduceand reduce collectives\&. "
.ti -1c
.RI "\fBCCL_ZE_IPC_EXCHANGE\fP"
.br
.RI "Set to specify the mechanism to use for Level Zero IPC exchange\&. "
.ti -1c
.RI "\fBCCL_ZE_DRM_BDF_SUPPORT\fP"
.br
.RI "Use bdf support for mapping logical to physical devices\&. "
.ti -1c
.RI "\fBCCL_REDUCE_SCATTER_FALLBACK_ALGO\fP"
.br
.RI "Use the fallback algorithm for reduce_scatter\&. "
.ti -1c
.RI "\fBCCL_ZE_AUTO_TUNE_PORTS\fP"
.br
.RI "Automatically tune algorithm protocols based on port count\&. "
.ti -1c
.RI "constexpr const char * \fBCCL_ZE_PT2PT_READ\fP = 'CCL_ZE_PT2PT_READ'"
.br
.RI "Enable switching of read and write protocols for pt2pt topo algorithm\&. "
.ti -1c
.RI "constexpr const char * \fBCCL_ZE_TYPE2_TUNE_PORTS\fP = 'CCL_ZE_TYPE2_TUNE_PORTS'"
.br
.RI "Tunable value for collectives to adjust copy engine indexes\&. "
.ti -1c
.RI "\fBCCL_BARRIER_SYNC\fP"
.br
.RI "Switch ccl::barrier() host-sync / host-async options\&. "
.ti -1c
.RI "\fBCCL_ENABLE_SYCL_KERNELS\fP"
.br
.RI "Enable SYCL kernels\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLGATHERV_TMP_BUF\fP"
.br
.RI "Enable the use of persistent temporary buffer in allgatherv\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLGATHERV_SMALL_THRESHOLD\fP"
.br
.RI "Specify the threshold for the small size algorithm in allgatherv\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLGATHERV_MEDIUM_THRESHOLD\fP"
.br
.RI "Specify the threshold for the medium size algorithm in allgatherv\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLGATHERV_SCALEOUT_THRESHOLD\fP"
.br
.RI "Specify the threshold for the scaleout algorithm in allgatherv \&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLREDUCE_TMP_BUF\fP"
.br
.RI "Enable the use of persistent temporary buffer in allreduce\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLREDUCE_SMALL_THRESHOLD\fP"
.br
.RI "Specify the threshold for the small size algorithm in allreduce\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLREDUCE_MEDIUM_THRESHOLD\fP"
.br
.RI "Specify the threshold for the medium size algorithm in allreduce\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLREDUCE_SCALEOUT_THRESHOLD\fP"
.br
.RI "Specify the maximum threshold for the Allreduce Sycl scale-out algorithm\&. "
.ti -1c
.RI "\fBCCL_SYCL_ALLREDUCE_SCALEOUT_DIRECT_THRESHOLD\fP"
.br
.RI "Specify the maximum threshold for the Allreduce Sycl scale-out direct algorithm\&. "
.ti -1c
.RI "\fBCCL_SYCL_REDUCE_SCATTER_TMP_BUF\fP"
.br
.RI "Enable the use of persistent temporary buffer in reduce_scatter\&. "
.ti -1c
.RI "\fBCCL_SYCL_REDUCE_SCATTER_SMALL_THRESHOLD\fP"
.br
.RI "Specify the threshold for the small size algorithm in reduce_scatter\&. "
.ti -1c
.RI "\fBCCL_SYCL_REDUCE_SCATTER_MEDIUM_THRESHOLD\fP"
.br
.RI "Specify the threshold for the medium size algorithm in reduce_scatter\&. "
.ti -1c
.RI "\fBCCL_SYCL_REDUCE_SCATTER_SCALEOUT_THRESHOLD\fP"
.br
.RI "Specify the threshold for the Sycl scaleout algorithm in reduce-scatter\&. "
.in -1c
.SH "Detailed Description"
.PP 
Experimental OneCCL Environment Variables Functionality of these variables has not been (fully) tested and, therefore, cannot be supported nor guaranteed\&. 


.SH "Variable Documentation"
.PP 
.SS "CCL_BARRIER_SYNC"

.PP
Switch ccl::barrier() host-sync / host-async options\&. Historically ccl::barrier() was always synchronous\&. That does not match with oneCCL asynchronous concept\&. Same as other collectives, ccl::barrier() should be host-asynchronous if possible\&. As it would be too much to change in one moment, we start through experimental variable which introduces the option to make barrier host-asynchronous\&. Use CCL_BARRIER_SYNC=0 to achieve that\&.
.PP
By-default: '1 (SYNC)' 
.SS "CCL_ENABLE_SYCL_KERNELS"

.PP
Enable SYCL kernels\&. Setting this environment variable to 1 enables SYCL kernel-based implementation for allgatherv, allreduce, and reduce_scatter\&. Support includes all message sizes and some data types (int32, fp32, fp16, and bf16), sum operation, and single node\&. oneCCL falls back to other implementations when the support is not available with SYCL kernels, so the user can safely setup this environment variable\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '0 (disabled)' 
.SS "CCL_REDUCE_SCATTER_FALLBACK_ALGO"

.PP
Use the fallback algorithm for reduce_scatter\&. The fallback algorithm performs a full allreduce and then copies a subset of its output to the recv buffer\&. Currently, the fallback algorithm is used for scaleout whereas scaleup uses optimized algorithm\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '0' 
.SS "CCL_REDUCE_SCATTER_MONOLITHIC_PIPELINE_KERNEL"

.PP
Set to specify monolithic pipeline approach for reduce_scatter phase in allreduceand reduce collectives\&. This enviroment variable has the advantage of forming a seamless pipeline that conceals the data transfer time across MDFI\&. This way, a process reads the data from its peer tile on the same GPU, performs the reduction, and writes to a temporary buffer located on a different GPU\&. This modification will cover the time for transferring the data through XeLinks during the reduce-scatter phase in allreduce and reduce collectives\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '1' 
.SS "CCL_SYCL_ALLGATHERV_MEDIUM_THRESHOLD"

.PP
Specify the threshold for the medium size algorithm in allgatherv\&. Set the threshold in bytes to specify the medium size algorithm in the allgatherv collective\&. Default value is 2097152\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_ALLGATHERV_SMALL_THRESHOLD"

.PP
Specify the threshold for the small size algorithm in allgatherv\&. Set the threshold in bytes to specify the small size algorithm in the allgatherv collective\&. Default value is 131072\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_ALLGATHERV_SCALEOUT_THRESHOLD"

.PP
Specify the threshold for the scaleout algorithm in allgatherv\&. Default value is 1048576\&. '<value>'' : '>=0"
.SS "CCL_SYCL_ALLGATHERV_TMP_BUF"

.PP
Enable the use of persistent temporary buffer in allgatherv\&. Setting this environment variable to 1 enables the use of a persistent temporary buffer to perform the allgatherv operation\&. This implementation makes the collective fully asynchronous but adds some additional overhead due to the extra copy of the user buffer to a (persistent) temporary buffer\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '0 (disabled)' 
.SS "CCL_SYCL_ALLREDUCE_MEDIUM_THRESHOLD"

.PP
Specify the threshold for the medium size algorithm in allreduce\&. Set the threshold in bytes to specify the medium size algorithm in the allreduce collective\&. Default value is 16777216\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_ALLREDUCE_SCALEOUT_DIRECT_THRESHOLD"

.PP
Specify the maximum threshold for the Allreduce Sycl scale-out direct algorithm\&. Set the threshold in bytes to specify the Sycl scaleout direct algorithm (call MPI_allreduce directly) in the allreduce collective\&. Default value is 1048576\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_ALLREDUCE_SCALEOUT_THRESHOLD"

.PP
Specify the maximum threshold for the Allreduce Sycl scale-out algorithm\&. Set the threshold in bytes to specify the Sycl scaleout algorithm in the allreduce collective\&. Default value is 1048576\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_ALLREDUCE_SMALL_THRESHOLD"

.PP
Specify the threshold for the small size algorithm in allreduce\&. Set the threshold in bytes to specify the small size algorithm in the allreduce collective\&. Default value is 524288\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_ALLREDUCE_TMP_BUF"

.PP
Enable the use of persistent temporary buffer in allreduce\&. Setting this environment variable to 1 enables the use of a persistent temporary buffer to perform the allreduce operation\&. This implementation makes the collective fully asynchronous but adds some additional overhead due to the extra copy of the user buffer to a (persistent) temporary buffer\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '0 (disabled)' 
.SS "CCL_SYCL_REDUCE_SCATTER_MEDIUM_THRESHOLD"

.PP
Specify the threshold for the medium size algorithm in reduce_scatter\&. Set the threshold in bytes to specify the medium size algorithm in the reduce_scatter collective\&. Default value is 67108864\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_REDUCE_SCATTER_SCALEOUT_THRESHOLD"

.PP
Specify the threshold for the Sycl scaleout algorithm in reduce-scatter\&. Set the threshold in bytes to specify the Sycl scaleout algorithm in the reduce-scatter collective\&. Default value is 4294967296\&. '<value>'' : '>=0" 
.SS "CCL_SYCL_REDUCE_SCATTER_SMALL_THRESHOLD"

.PP
Specify the threshold for the small size algorithm in reduce_scatter\&. Set the threshold in bytes to specify the small size algorithm in the reduce_scatter collective\&. Default value is 2097152\&.'<value>'' : '>=0" 
.SS "CCL_SYCL_REDUCE_SCATTER_TMP_BUF"

.PP
Enable the use of persistent temporary buffer in reduce_scatter\&. Setting this environment variable to 1 enables the use of a persistent temporary buffer to perform the reduce_scatter operation\&. This implementation makes the collective fully asynchronous but adds some additional overhead due to the extra copy of the user buffer to a (persistent) temporary buffer\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '0 (disabled)' 
.SS "CCL_ZE_AUTO_TUNE_PORTS"

.PP
Automatically tune algorithm protocols based on port count\&. Use number of ports to detect the 12 ports system and use write protocols on such systems for collectives\&. Users can disable this automatic detection and select the protocols manually\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '1' 
.SS "CCL_ZE_DRM_BDF_SUPPORT"

.PP
Use bdf support for mapping logical to physical devices\&. To obtain the physical device id based on the bdf, we need get and then parse the bdf values\&. Then using those values we can identify the particular device by referencing the appropriate fields in a pci configuration space for pci devices\&.to utilize bdf for the purpose of mapping logical devices to their corresponding physical devices\&.
.PP
'<value>' : '0', '1'
.PP
By-default: '1' 
.SS "CCL_ZE_IPC_EXCHANGE"

.PP
Set to specify the mechanism to use for Level Zero IPC exchange\&. 
.br
 'drmfd' - Uses a the DRM mechanism for Level Zero IPC exchange\&. This is an experimental mechanism that is used with OS kernels previous to SP4\&. To use the DRM mechanism, the libdrm and drm headers must be available on a system\&. 
.br
 'pidfd' - Uses pidfd mechanism for Level Zero IPC exchange\&. It requires OS kernel SP4 or above as it requires Linux 5\&.6 kernel or above 
.br
 'sockets' - Uses socket mechanism for Level Zero IPC exchange\&. It is usually slower than the other two mechanisms, but can be used for debugging as it is usually available on most systems
.PP
'<value>': 'drmfd', 'pidfd', 'sockets'
.PP
By-default: 'pidfd' 
.SS "constexpr const char* CCL_ZE_PT2PT_READ = 'CCL_ZE_PT2PT_READ'\fC [constexpr]\fP"

.PP
Enable switching of read and write protocols for pt2pt topo algorithm\&. Control pt2pt read/write protocols\&.
.br
 Read Protocol:
.br
 It means SEND side is exchanging the handle with RECV side\&. Then execute the copy operation on the RECV operation side, where the dst buf is the local buffer and the source buffer is the remote buffer\&.
.br
 Write Protocol:
.br
 it means RECV side is exchanging the handle with SEND side\&. Execute the copy operation on the SEND operation side, where the dst buf is the remote buffer and the source buffer is the local buffer\&. 
.br
 '<value>' : '0', '1' 
.br
 By-default: '1' 
.SS "constexpr const char* CCL_ZE_TYPE2_TUNE_PORTS = 'CCL_ZE_TYPE2_TUNE_PORTS'\fC [constexpr]\fP"

.PP
Tunable value for collectives to adjust copy engine indexes\&. use 2,4,6 copy engine indexes for host with 6 ports for allreduce, reduce and allgatherv '<value>': 'on' - always use write mode with calculated indexes 'off' - always disabled 'detected' - determined by the logic in detection 'undetected' - the default value, used before the logic in detection
.PP
By-default: 'undetected' 
.SH "Author"
.PP 
Generated automatically by Doxygen for IntelÂ® oneAPI Collective Communications Library from the source code\&.
